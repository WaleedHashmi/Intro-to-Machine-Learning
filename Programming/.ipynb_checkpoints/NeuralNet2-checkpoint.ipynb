{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming session 3: Neural Networks, Part II "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Your first convolutional neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this exercise, we will build convolutional neural networks with Keras and apply them to various datasets.\n",
    "Go oline and download the CIFAR 10 dataset: https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While downloading the CIFAR 10 dataset, we will train a first neural network on the simpler CIFAR 10 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3.1.1.__ Use the following lines to load the training and test data. Then display a couple of images using the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys,pickle\n",
    "import gzip\n",
    "f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "if sys.version_info < (3,):\n",
    "    data = pickle.load(f)\n",
    "else:\n",
    "    data = pickle.load(f, encoding='bytes')\n",
    "f.close()\n",
    "(x_train, y_train), (x_test, y_test) = data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.datasets import mnist\n",
    "# import os\n",
    "\n",
    "# # Setup train and test splits\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PIL'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-9f4bfd2b0cec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# np.shape(x_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'PIL'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# np.shape(x_train)\n",
    "\n",
    "plt.imshow(x_train, interpolation='nearest')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the training and test set and flatten the images so that your data matrix is of size $N_{\\text{images}} * n^2$ where $n$ is the number of rows/columns of each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3.1.2.__ To train our neural network, the first thing we will do is turn our labels into one hot encoding. That is binary vectors of the form $(1,0,\\ldots, 0)$ where the $i^{th}$ entry is $1$ if the digit represented on the figure is the digit $i$ and $0$ otherwise. We can either do this manually, or we can use the built in function from Keras. \n",
    "\n",
    "- Start by writing a small routine that manually turns the labels from y_train and y_test into arrays of one hot encodings. We want the output to be an array of size $N_1\\times K$ where $N-1$ is the number of points in the training and test sets and $K$ is the number of distinct classes in y. \n",
    "\n",
    "- One alternative is to use the built in implementation from Keras. Keras provide the function 'to_categorical' which takes as argument any vector of labels and return the one hot encoding.   \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put your \"manual\" routine here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Alternatively, use the Keras implementation here\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3.1.3.__ We will use the class 'Sequential' from Keras together with dense (i.e fully connected layers). Building a neural network is easy with Keras. We initialize the model with the line 'model = Sequential()' then we use model.add(). To start we will build a simple one hidden layer neural network. Use the function 'add' to add two layers:\n",
    " - The first layer should be a dense layer with 32 neurons, each with a sigmoid activation function. This layer should take as input a vectorized image (hence the input size should be the size of your image, $n_{\\text{row}} \\times  n_{\\text{cols}}$)\n",
    " \n",
    " - The second Output layer, should be dense and output a vector with $K$ elements (K corresponding to the number of classes). Which activation function should we choose according to you? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.layers import Dense # Dense layers are \"fully connected\" layers\n",
    "from keras.models import Sequential # Documentation: https://keras.io/models/sequential/\n",
    "\n",
    "\n",
    "# put your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Exercise 3.1.4.__ Once your model is set up, you can use the function .summary() to display its features. What is the total number of parameters of your model? To train the model, we first need to specify which loss and which solver we want to use. This can be done with the compile function (check the documentation of the function ). Set \"sgd\" as your solver. The loss is what is optimized by the SGD algorithm when you train your network. As we deal with binary variables, \"categorical_crossentropy\" is a good idea. Finally, through the iterations, we might want to monitor the improvement of the model with respect to some criterion. You can specify such a criterion through the \"metrics\" argument of your call to compile. The \"metrics\" argument enables you to define an additional function to monitor through the iterations. We could for example display the the average number of misclassified images. This is can be done with metrics=['accuracy']\n",
    "\n",
    "   - Call the function \"compile\"of your model with the parameters specified above (this should really be one line)\n",
    "\n",
    "   - Now that the optimization is set up, we will train the model. To do this, we for other machine learning models in Keras, we use the function fit. Check the documentation of the function. For this function we need to specify a couple of arguments. The first two arguments are the training samples and labels. Now is also a good time to introduce the notions of batch and epochs. As we discussed before the parameter 'batch_size' is the number of (training) samples we use at each step of the stochastic gradient descent. The fourth parameter \"epochs = ...\" indicate the number of times you want to go through the whole dataset. \n",
    "    \n",
    "    You can also play with verbose to display more or less information on the training of your network\n",
    "    \n",
    "    To get an intuition on the improvement of your model you could decide to train it on a subset of the data and keep a small fraction of the data to evaluate the generalization ability of your model throughout the iterations. This is done by setting the parameter \"validation_split\" to some fraction of the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put your code here\n",
    "\n",
    "\n",
    "history = model.fit(...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3.1.5.__ By using a call such as the one above, you get to keep track of the validation errors through the training of the model. \n",
    "Use the data stored in \"history.history['acc']\" and \"history.history['val_acc']\" to plot the evolution of the training and validation accuracy of your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put your code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3.1.6__ We now want to study how weel other network architectures would perform on the same dataset. Write a simple function that takes as input an array with a number $N_{h}$ of layers and a number of neurons $N_{L,n}$ per layers. The function should return the same neural network as before except that we now want the architecture to include $N_{h}$ additional layers and $N_{L,n}$ neurons in each layers. You can choose the activation as you want (sigmoid is a good idea). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NNdesign(layer_sizes):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return NeuralNet  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3.1.7__ Use your function (together with the short code you wrote to set up the optimizer and train your network -- keep the same parameters for now) and display the evolutions of the training and validation errors through training for neural networks with 32 neurons and 1 to 4 hidden layers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put your code here \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3.1.8__ Use your three layers neural network and increase the number of epochs to 40. What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put your code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is your conclusion in terms of depth ? What is important to train a deep neural network correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "What is your conclusion in terms of depth ? What is important to train a deep neural network correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3.1.9__ Now instead of keeping the number of neurons constant for a varying number of layers, repeat your experiments with a single layer and study how the training and validation errors evolve when considering various number of neurons (take 32 68 and 128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 A slightly more interesting dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should be done with the CIFAR 10 download. Training a neural net on CIFAR is a little more involved but follows the same idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Simple CNN model for CIFAR-10\n",
    "import numpy\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3.2.1__ Load the data using the lines below and display some of the images and the associated labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building and training the neural net, we use the normalize the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3.2.2__ Turn the labels into categorical variables as we did in MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3.2.3__ Build your neural network with Keras following the guidelines below\n",
    "\n",
    "- We want a sequential model\n",
    "- First layer should be a 2d convolutional layer with 32 '3 by 3' filters\n",
    "- Add a Dropout layer with 2% dropout (what is the effect of such a line?)\n",
    "- Then add again a convolutional layer with 32 '3 by 3' filters\n",
    "- Add the following line: model.add(MaxPooling2D(pool_size=(2, 2))). What is the effect of such a line?\n",
    "- Flatten the output\n",
    "- Then add a Desne layer of 512 units with relu activations. \n",
    "- Add another dropout layer (50% dropout)\n",
    "- Add the output layer (Dense, num of output = num of possible classes and activation = softmax)\n",
    "\n",
    "In the Dense and Convolutional layer, you might want to regularize by passing the additional argument \"kernel_constraint=maxnorm(3)\"\n",
    "\n",
    "\n",
    "Use the following parameters to set up the optimization with the compile function: \n",
    "\n",
    "epochs = 25\n",
    "lrate = 0.01\n",
    "decay = lrate/epochs\n",
    "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Build your model below\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
